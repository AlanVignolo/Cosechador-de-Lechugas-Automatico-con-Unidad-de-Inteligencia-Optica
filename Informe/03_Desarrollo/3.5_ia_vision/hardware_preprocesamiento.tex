\subsubsection{Sistema de Adquisición Visual}

El hardware de adquisición de imágenes constituye el elemento sensorial primario del sistema de inteligencia artificial. La selección del dispositivo de captura se basó en criterios de resolución, frecuencia de muestreo, compatibilidad con la plataforma de procesamiento y costo.

Se implementó una cámara USB Logitech C920 HD Pro que proporciona captura de imágenes con resolución máxima de 1920×1080 píxeles a una tasa de 30 cuadros por segundo. El sensor de imagen tipo CMOS de 1/2.7 pulgadas presenta sensibilidad adecuada para las condiciones de iluminación del invernadero, con valores típicos de 800 a 1200 lux proporcionados por iluminación LED blanca de 5000K instalada en el techo de la estructura.

El campo de visión de 78 grados en diagonal permite capturar un área aproximada de 280×160 milímetros a la distancia de trabajo nominal de 200 milímetros. Esta configuración óptica resulta en una resolución espacial de 0.146 mm/píxel en dirección horizontal y 0.148 mm/píxel en dirección vertical, calculadas como el cociente entre las dimensiones físicas del área capturada y la resolución en píxeles correspondiente.

La cámara incorpora sistema de enfoque automático que ajusta la posición de las lentes para mantener la nitidez de la imagen cuando varía la distancia de trabajo. Este mecanismo es crítico dado que el robot opera en un rango de distancias de 150 a 250 milímetros durante las diferentes fases de operación. El tiempo de convergencia del autofocus es inferior a 2 segundos cuando se produce un cambio significativo de distancia.

El montaje de la cámara se realizó mediante un soporte diseñado específicamente e impreso en material PLA mediante manufactura aditiva. El soporte se fija al extremo del brazo robótico y permite ajuste angular de ±15 grados en los ejes de cabeceo y guiñada. Esta capacidad de ajuste es necesaria para compensar tolerancias de ensamblaje y optimizar la orientación del campo de visión respecto al plano de cultivo.


La interfaz entre la cámara y la Raspberry Pi se establece mediante conexión USB 2.0, que proporciona ancho de banda suficiente para la transferencia de imágenes sin compresión a la tasa de captura requerida. El controlador Video4Linux2, incluido nativamente en el sistema operativo, gestiona la comunicación con el dispositivo y expone las funciones de captura mediante una interfaz de programación estándar.

La latencia total del sistema de adquisición, medida desde la solicitud de captura hasta la disponibilidad de la imagen en memoria, comprende cuatro componentes: tiempo de exposición del sensor (típicamente 8 a 15 milisegundos dependiendo de las condiciones de iluminación), tiempo de lectura del array de píxeles (aproximadamente 8 milisegundos), tiempo de transferencia USB (15 milisegundos para una imagen de 6.2 megabytes), y tiempo de decodificación en el procesador host (10 milisegundos). La latencia total resultante es de aproximadamente 45 a 50 milisegundos.

\subsubsection{Preprocesamiento de Imágenes}

Las imágenes capturadas requieren transformaciones preliminares que optimizan su procesamiento posterior y mejoran la robustez del sistema ante variaciones ambientales. El preprocesamiento constituye la etapa de acondicionamiento de señal del pipeline de visión artificial.

La primera operación de preprocesamiento consiste en la conversión del espacio de color RGB nativo de la cámara al espacio HSV. Esta transformación aplica las ecuaciones de conversión estándar presentadas en el marco teórico (Sección 2.2.1), calculando para cada píxel los valores de matiz, saturación y valor a partir de las componentes rojo, verde y azul. La separación de información cromática e información de luminosidad que proporciona esta representación resulta fundamental para la robustez del sistema.

El módulo de posicionamiento visual explota esta separación extrayendo únicamente el canal V, que representa el brillo o luminosidad de cada píxel independientemente de su color. Esta estrategia proporciona invariancia ante cambios en la tonalidad de la iluminación, permitiendo la detección de cintas negras de referencia incluso cuando el espectro de la fuente luminosa varía. El canal V se extrae mediante indexación directa del tercer plano de la imagen transformada.

El módulo de clasificación de cultivos, por el contrario, emplea los tres canales simultáneamente para realizar segmentación por rango cromático. Esta operación evalúa para cada píxel si sus valores de matiz, saturación y valor se encuentran dentro de límites predefinidos que corresponden a las tonalidades verdes características de la vegetación. Los píxeles que cumplen esta condición se marcan en una máscara binaria, aislando efectivamente las regiones de interés del resto de la escena.

Para optimizar el rendimiento computacional se implementa recorte adaptativo de región de interés. Este mecanismo limita el procesamiento a una ventana rectangular centrada en la posición esperada del elemento a detectar, reduciendo significativamente el número de píxeles que deben procesarse. Las dimensiones de la región de interés se determinan en función de la posición actual del robot y del conocimiento a priori de la geometría del sistema. Las coordenadas de los límites de la región se calculan mediante:

\begin{align}
x_{min} &= \max(0, x_{centro} - w_{ROI}/2) \\
x_{max} &= \min(W_{imagen}, x_{centro} + w_{ROI}/2) \\
y_{min} &= \max(0, y_{centro} - h_{ROI}/2) \\
y_{max} &= \min(H_{imagen}, y_{centro} + h_{ROI}/2)
\end{align}

donde $W_{imagen}$ y $H_{imagen}$ son las dimensiones totales de la imagen capturada, $(x_{centro}, y_{centro})$ es la posición estimada del elemento de interés, y $w_{ROI}$ y $h_{ROI}$ son las dimensiones de la región de interés. Las funciones de máximo y mínimo garantizan que los límites no excedan los bordes de la imagen.

Esta estrategia reduce el área de procesamiento en aproximadamente 65 por ciento, mejorando proporcionalmente los tiempos de respuesta del sistema. El recorte se ejecuta mediante operaciones de indexación de arrays que no implican copia de datos, resultando en un costo computacional despreciable.

\subsubsection{Segmentación por Umbralización}

La segmentación constituye la operación fundamental que separa las regiones de interés del fondo de la imagen. Se implementan dos estrategias de umbralización adaptadas a los requerimientos específicos de cada módulo.

Para el módulo de posicionamiento visual se aplica umbralización inversa sobre el canal V del espacio HSV. Esta operación compara el valor de brillo de cada píxel contra un umbral de referencia, asignando valor máximo (255) a los píxeles cuyo brillo es inferior al umbral, y valor nulo (0) a los píxeles con brillo superior. Matemáticamente:

\begin{equation}
I_{bin}(x,y) = \begin{cases}
255 & \text{si } I_V(x,y) < T \\
0 & \text{si } I_V(x,y) \geq T
\end{cases}
\end{equation}

El valor de umbral $T = 50$ se determinó empíricamente mediante análisis del histograma de intensidades en imágenes representativas del entorno operativo. Este valor maximiza la separación entre las cintas negras de referencia y el fondo blanco del sistema hidropónico, minimizando simultáneamente la tasa de falsos positivos.

Para el módulo de clasificación se emplea segmentación por rango en el espacio tridimensional HSV. Cada píxel se evalúa contra límites inferior y superior en los tres canales, asignándose a la región de interés únicamente si todas las componentes se encuentran dentro del rango especificado:

\begin{equation}
M(x,y) = \begin{cases}
255 & \text{si } H_{min} \leq H(x,y) \leq H_{max} \\
& \land \, S_{min} \leq S(x,y) \leq S_{max} \\
& \land \, V_{min} \leq V(x,y) \leq V_{max} \\
0 & \text{en caso contrario}
\end{cases}
\end{equation}

Los límites se establecieron como $H_{min} = 25^\circ$, $H_{max} = 85^\circ$ para cubrir el espectro de verdes desde tonalidades amarillentas hasta azuladas; $S_{min} = 40$ para eliminar colores desaturados del fondo que podrían presentar componente verde débil; y $V_{min} = 40$ para descartar sombras muy oscuras. Los límites superiores de saturación y valor se fijan en sus valores máximos (255).

\subsubsection{Refinamiento Morfológico}

Las máscaras binarias resultantes de la segmentación contienen típicamente ruido aislado y discontinuidades en las regiones de interés debido a variaciones locales de iluminación, reflexiones especulares y limitaciones del sensor. Se aplica una secuencia de operaciones morfológicas para refinar estas máscaras.

La primera operación corresponde al cierre morfológico, que elimina pequeños huecos dentro de las regiones y conecta componentes próximos. Matemáticamente, el cierre se define como una dilatación seguida de una erosión:

\begin{equation}
M_{cierre} = (M \oplus K) \ominus K
\end{equation}

donde $M$ es la máscara binaria de entrada, $K$ es el elemento estructurante, $\oplus$ denota dilatación y $\ominus$ denota erosión. La dilatación expande las regiones blancas, rellenando huecos pequeños, mientras que la erosión subsecuente restaura aproximadamente el tamaño original de las regiones.

La segunda operación corresponde a la apertura morfológica, que elimina píxeles aislados y pequeñas protuberancias. Se define como una erosión seguida de una dilatación:

\begin{equation}
M_{apertura} = (M_{cierre} \ominus K) \oplus K
\end{equation}

La erosión elimina estructuras pequeñas que no pueden contener completamente el elemento estructurante, y la dilatación subsecuente restaura el tamaño de las estructuras remanentes.

El elemento estructurante empleado es una matriz rectangular de 3×3 píxeles con todos sus elementos igual a uno. Este tamaño se seleccionó como compromiso entre capacidad de eliminación de ruido y preservación de detalles relevantes. Elementos estructurantes mayores eliminarían ruido más efectivamente pero podrían degradar características geométricas importantes de las regiones.

El resultado de estas operaciones morfológicas es una máscara binaria refinada donde las regiones de interés presentan fronteras suaves y conectividad mejorada, facilitando la subsecuente detección de contornos.

\subsubsection{Detección y Análisis de Contornos}

Sobre la máscara refinada se ejecuta el algoritmo de detección de contornos de Suzuki-Abe, cuyo fundamento teórico se presentó en la Sección 2.2.2 del marco teórico. Este algoritmo recorre la imagen binaria identificando las fronteras entre regiones blancas y negras, construyendo una representación vectorial de cada contorno como secuencia ordenada de coordenadas de píxeles.

Se emplea el modo de extracción de contornos externos únicamente, que detecta solamente el contorno exterior de cada región conexa ignorando posibles huecos internos. Esta configuración es apropiada dado que los objetos de interés (lechugas y cintas de referencia) no presentan estructuras anidadas relevantes para la aplicación. La aproximación de contornos se realiza mediante compresión de segmentos colineales, donde secuencias de puntos que forman líneas rectas se representan únicamente por sus extremos. Esta estrategia reduce significativamente el uso de memoria y acelera operaciones posteriores sobre los contornos.

Los contornos detectados se someten a filtrado para eliminar aquellos que no corresponden a objetos de interés. El criterio principal de filtrado es el área, calculada como el número de píxeles encerrados por el contorno. Para el módulo de posicionamiento se descartan contornos con área inferior a 500 píxeles, mientras que para el módulo de clasificación el umbral es de 5000 píxeles. Estos valores se establecieron considerando el tamaño esperado de los objetos de interés a la distancia de trabajo nominal.

Para cada contorno que supera el filtrado se calculan descriptores geométricos que cuantifican sus características. El área se obtiene mediante conteo de píxeles interiores empleando el algoritmo de relleno por escaneo de líneas. El centroide se calcula mediante los momentos de imagen de orden cero y uno:

\begin{equation}
c_x = \frac{M_{10}}{M_{00}}, \quad c_y = \frac{M_{01}}{M_{00}}
\end{equation}

donde $M_{ij}$ representa el momento de orden $i+j$ definido como:

\begin{equation}
M_{ij} = \sum_{(x,y) \in \text{Región}} x^i y^j
\end{equation}

El perímetro se calcula como la suma de distancias euclidianas entre puntos consecutivos del contorno. El rectángulo delimitador se determina identificando las coordenadas mínimas y máximas en ambas direcciones.

Cuando múltiples contornos cumplen los criterios de filtrado, se implementa un sistema de selección del contorno principal. Para el módulo de posicionamiento se emplea scoring ponderado que considera simultáneamente el área del contorno, su posición relativa al centro de la imagen, y la calidad de su región basal. Para el módulo de clasificación se selecciona simplemente el contorno de mayor área, bajo la hipótesis de que la lechuga constituye el objeto verde dominante en la escena.

El tiempo total de procesamiento de la etapa de detección y análisis de contornos es de aproximadamente 40 milisegundos, incluyendo la detección, el filtrado y el cálculo de descriptores. Este tiempo es compatible con los requerimientos de latencia del sistema de control en tiempo real.