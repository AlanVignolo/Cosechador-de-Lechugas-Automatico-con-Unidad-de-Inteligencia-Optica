\subsection{Clasificación Basada en Análisis Morfológico}

La clasificación morfológica de objetos utiliza características geométricas y estadísticas extraídas de contornos e imágenes para asignar categorías a elementos detectados. A diferencia de métodos de aprendizaje profundo que requieren extensos conjuntos de entrenamiento y elevada capacidad computacional, este enfoque se fundamenta en reglas explícitas derivadas del conocimiento del dominio y análisis estadístico de muestras representativas. El método resulta particularmente efectivo cuando las clases presentan diferenciación clara en características medibles como tamaño, forma o color, y cuando se dispone de entornos controlados donde las condiciones de captura son predecibles.

\subsubsection{Clasificación por Umbrales sobre Descriptores}

El método más directo establece límites de decisión basados en valores de descriptores geométricos. Para un descriptor $d$ y un conjunto de clases $\mathcal{C} = \{c_1, c_2, ..., c_n\}$, la regla de clasificación se define mediante umbrales $t_0, t_1, ..., t_n$ que particionan el espacio de características:

\begin{equation}
\text{Clase}(x) = c_i \quad \text{si} \quad t_{i-1} < d(x) \leq t_i
\end{equation}

Para objetos que se diferencian principalmente por tamaño, el área en píxeles constituye un descriptor efectivo. Por ejemplo, en clasificación de tubos de cultivo donde vasos vacíos presentan poca vegetación visible, plantines muestran área verde intermedia, y lechugas desarrolladas ocupan áreas grandes, se establecen umbrales que separan estas categorías. La robustez del método depende de la separabilidad entre clases: la distancia entre centroides de clases adyacentes debe ser significativa respecto a la dispersión intra-clase para minimizar errores de clasificación.

La selección óptima de umbrales requiere análisis estadístico de una base de datos representativa. Para cada clase $c_i$, se calculan la media $\mu_i$ y desviación estándar $\sigma_i$ del descriptor mediante:

\begin{equation}
\mu_i = \frac{1}{N_i}\sum_{j=1}^{N_i} d_j, \quad \sigma_i = \sqrt{\frac{1}{N_i-1}\sum_{j=1}^{N_i}(d_j - \mu_i)^2}
\end{equation}

donde $N_i$ es el número de muestras de la clase $i$ y $d_j$ los valores del descriptor observados. Asumiendo distribuciones normales con varianzas similares, el umbral óptimo entre clases consecutivas que minimiza la probabilidad de error es el punto medio entre sus medias:

\begin{equation}
t_i^* = \frac{\mu_i + \mu_{i+1}}{2}
\end{equation}

Este criterio resulta de igualar las densidades de probabilidad gaussianas de ambas clases y resolver para el punto de intersección.

\subsubsection{Clasificación Multi-Criterio con Distancia Normalizada}

Cuando un único descriptor no proporciona separabilidad suficiente, se emplean múltiples características simultáneamente. En agricultura de precisión, los ratios de color resultan particularmente efectivos. Dado un contorno detectado, se calculan proporciones de píxeles de diferentes tonalidades dentro de su región:

\begin{equation}
r_{verde} = \frac{N_{píxeles\_verdes}}{N_{píxeles\_totales}}, \quad r_{negro} = \frac{N_{píxeles\_negros}}{N_{píxeles\_totales}}
\end{equation}

Cada objeto se representa como un vector en el espacio de características $\mathbf{x} = [r_{verde}, r_{negro}, I_{media}]^T$, donde $I_{media}$ es la intensidad promedio de píxeles internos. Para cada clase $c_i$, se define un prototipo mediante las medias de los descriptores $\boldsymbol{\mu}_i = [\mu_{verde,i}, \mu_{negro,i}, \mu_{intensidad,i}]^T$ y desviaciones estándar $\boldsymbol{\sigma}_i = [\sigma_{verde,i}, \sigma_{negro,i}, \sigma_{intensidad,i}]^T$ calculadas sobre el conjunto de entrenamiento.

La clasificación de un objeto nuevo se realiza calculando su distancia normalizada a cada prototipo de clase. La normalización por desviación estándar garantiza que descriptores con diferentes rangos contribuyan equitativamente:

\begin{equation}
D_i(\mathbf{x}) = \sqrt{\sum_{k=1}^{m} \left(\frac{x_k - \mu_{k,i}}{\sigma_{k,i} + \epsilon}\right)^2}
\end{equation}

donde $m$ es el número de descriptores, $x_k$ el valor del descriptor $k$ para el objeto, $\mu_{k,i}$ y $\sigma_{k,i}$ los parámetros de la clase $i$, y $\epsilon$ un término pequeño ($10^{-6}$) que evita división por cero. Esta métrica implementa la distancia de Mahalanobis simplificada bajo la asunción de independencia entre descriptores. El objeto se asigna a la clase con menor distancia:

\begin{equation}
\text{Clase}(\mathbf{x}) = \arg\min_{i} D_i(\mathbf{x})
\end{equation}

La confianza de la clasificación se estima mediante la relación entre distancias:

\begin{equation}
\text{Confianza} = 1 - \frac{D_{min}}{D_{max} + D_{min}}
\end{equation}

donde $D_{min}$ es la distancia a la clase predicha y $D_{max}$ la distancia a la clase más lejana. Valores cercanos a 1 indican alta confianza (objeto claramente perteneciente a una clase); valores cercanos a 0.5 sugieren ambigüedad (objeto equidistante entre clases). Esta información de confianza puede emplearse en el nivel supervisor para implementar estrategias de verificación adicional cuando la clasificación es incierta.

\subsubsection{Ventajas y Limitaciones del Enfoque}

El enfoque morfológico presenta ventajas significativas para aplicaciones de agricultura de precisión. La interpretabilidad de las reglas de decisión facilita la validación agronómica y la depuración de fallos mediante inspección directa de valores de descriptores. La eficiencia computacional permite ejecución en tiempo real sobre hardware embebido sin requerir GPUs o aceleradores especializados. El método requiere únicamente decenas a cientos de muestras por clase para establecer parámetros estadísticos, en contraste con los miles de imágenes necesarios para entrenar redes neuronales profundas. El control explícito sobre umbrales y pesos permite ajuste directo según requerimientos operativos, sin necesidad de reentrenamiento completo del modelo.

Las limitaciones incluyen la necesidad de diseño manual de características, requiriendo conocimiento experto del dominio para identificar descriptores relevantes. El método se limita a características directamente medibles, sin capacidad de capturar patrones visuales complejos o texturas sutiles que podrían ser aprendidas automáticamente por métodos de aprendizaje profundo. Variaciones significativas de iluminación o perspectiva pueden afectar los descriptores cromáticos, aunque este efecto se mitiga en entornos controlados como invernaderos con iluminación artificial. La separabilidad lineal de clases en el espacio de características limita la aplicabilidad: el método funciona efectivamente con 2-5 categorías bien diferenciadas, pero puede degradar con mayor número de clases o cuando las clases no son linealmente separables.

El enfoque resulta particularmente adecuado para clasificación de cultivos hidropónicos cuando existe diferenciación clara por tamaño, forma o color (como plantines vs. plantas maduras vs. tubos vacíos), el entorno es controlado, se requiere operación en hardware con recursos limitados, la interpretabilidad es crítica para validación, y se dispone de conocimiento experto para diseñar descriptores relevantes. Bajo estas condiciones, la combinación con análisis estadístico robusto permite alcanzar exactitudes superiores a 90\% sin la complejidad y requisitos computacionales de métodos de aprendizaje profundo.
